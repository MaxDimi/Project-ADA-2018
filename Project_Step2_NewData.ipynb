{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing IRA tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directory\n",
    "DATA_DIR = 'data_new/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting to know our main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'iran_troll_tweet_text.csv')\n",
    "tweets_stats_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'iran_troll_tweet_stats.csv')\n",
    "tweets_meta_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'iran_troll_tweet_metadata.csv')\n",
    "tweets_user_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'iran_troll_user.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1122936, 3)\n",
      "+------------------+--------------+--------------------+\n",
      "|           tweetid|tweet_language|          tweet_text|\n",
      "+------------------+--------------+--------------------+\n",
      "|533622371429543936|            fr|@bellisarobz Ces ...|\n",
      "|527205814906654721|            en|@ParkerLampe An i...|\n",
      "|545166827350134784|            en|@hadeelhmaidi @wo...|\n",
      "|538045437316321280|            fr|@MartinYannis l'a...|\n",
      "|530053681668841472|            fr|@courrierinter Le...|\n",
      "|479670430911836160|            en|@irfhabib why bok...|\n",
      "|526450009382719488|            en|@KhushbuCNN ISIS ...|\n",
      "|525593430731157504|            en|@placesbrands Tur...|\n",
      "|617751516947030016|            fr|@fentychuck L’Uni...|\n",
      "|612565807911030784|            fr|@PringlesNico pou...|\n",
      "+------------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((tweets_text_df.count(), len(tweets_text_df.columns)))\n",
    "tweets_text_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown *tweet_language* can take both the value 'und', or null. We harmonize this column by setting all NaN to 'und'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_df = tweets_text_df.fillna('und',['tweet_language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1122936, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'),\n",
       " ('userid', 'string'),\n",
       " ('tweet_time', 'string'),\n",
       " ('in_reply_to_tweetid', 'string'),\n",
       " ('in_reply_to_userid', 'string'),\n",
       " ('quoted_tweet_tweetid', 'string'),\n",
       " ('is_retweet', 'string'),\n",
       " ('retweet_userid', 'string'),\n",
       " ('retweet_tweetid', 'string'),\n",
       " ('quote_count', 'string'),\n",
       " ('reply_count', 'string'),\n",
       " ('like_count', 'string'),\n",
       " ('retweet_count', 'string'),\n",
       " ('hashtags', 'string'),\n",
       " ('urls', 'string'),\n",
       " ('user_mentions', 'string'),\n",
       " ('poll_choices', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((tweets_stats_df.count(), len(tweets_stats_df.columns)))\n",
    "tweets_stats_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the *tweet_time* into Datetime for ease of use, and we cast some columns into integers. We also create a static sql view of the main dataframe on which we can apply our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stats_df = tweets_stats_df.withColumn('tweet_time', to_timestamp(tweets_stats_df.tweet_time))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('quote_count', tweets_stats_df.quote_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('reply_count', tweets_stats_df.reply_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('like_count', tweets_stats_df.like_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('retweet_count', tweets_stats_df.retweet_count.cast('int'))\n",
    "tweets_stats_df.createOrReplaceTempView(\"tweets_stats_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start splitting the data into smaller dataframes and remove the useless columns for each of those:\n",
    "* **retweets_df** contains all the posts that are retweets.\n",
    "* **replies_df** contains all the posts that are replies to other tweets.\n",
    "* **normal_tweets_df** contains all the other ('normal') posts.\n",
    "\n",
    "**NB:** some tweets have a value for *in_reply_to_userid* while their *in_reply_to_tweetid* is null (however the inverse never happens). Those are either replies to deleted tweets, or mentions of other users that were treated as replies. We decided to consider them as normal tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232337, 9)\n"
     ]
    }
   ],
   "source": [
    "# RETWEETS\n",
    "retweets_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=True\")\n",
    "\n",
    "# To understand how we selected the columns to remove, uncomment the next two lines\n",
    "#for col in retweets_df:\n",
    "#    retweets_df.select(col).distinct().show()\n",
    "\n",
    "retweets_df = retweets_df.drop('in_reply_to_tweetid', 'in_reply_to_userid', 'is_retweet', 'quote_count', 'reply_count', 'like_count', 'retweet_count', 'poll_choices')\n",
    "print((retweets_df.count(), len(retweets_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339350, 14)\n"
     ]
    }
   ],
   "source": [
    "# REPLIES\n",
    "replies_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=False AND in_reply_to_tweetid IS NOT NULL\")\n",
    "replies_df = replies_df.drop('retweet_tweetid', 'retweet_userid', 'is_retweet')\n",
    "print((replies_df.count(), len(replies_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(551249, 13)\n"
     ]
    }
   ],
   "source": [
    "# NORMAL\n",
    "normal_tweets_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=False AND in_reply_to_tweetid IS NULL\")\n",
    "normal_tweets_df = normal_tweets_df.drop('in_reply_to_tweetid', 'retweet_tweetid', 'retweet_userid', 'is_retweet')\n",
    "print((normal_tweets_df.count(), len(normal_tweets_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the number of rows correspond and that we did not duplicate or remove any by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122936 1122936\n"
     ]
    }
   ],
   "source": [
    "print(tweets_stats_df.count(), retweets_df.count()+normal_tweets_df.count()+replies_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1122936, 6)\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "|           tweetid|follower_count|following_count|latitude|longitude| tweet_client_name|\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "|533622371429543936|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|527205814906654721|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|545166827350134784|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|538045437316321280|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|530053681668841472|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|479670430911836160|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|526450009382719488|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|525593430731157504|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|617751516947030016|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "|612565807911030784|          8012|           1450|    null|     null|Twitter Web Client|\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((tweets_meta_df.count(), len(tweets_meta_df.columns)))\n",
    "tweets_meta_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the number of rows with a non-null *latitude*/*longitude* combination is very small compared to the size of dataset. Furthermore, several of them are repeated. We thus consider it rather useless and prefer dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122936 32 20\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT * FROM tweets_meta_sql WHERE latitude IS NOT NULL\")\n",
    "print(tweets_meta_df.count(), temp.count(), temp.select('latitude', 'longitude').distinct().count())\n",
    "\n",
    "tweets_meta_df = tweets_meta_df.drop('latitude', 'longitude')\n",
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122936 699348\n"
     ]
    }
   ],
   "source": [
    "# Possibility to discriminate on tweet_client_name\n",
    "temp = spark.sql(\"SELECT * FROM tweets_meta_sql WHERE tweet_client_name='Twitter Web Client'\")\n",
    "print(tweets_meta_df.count(), temp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Check that it makes sense with the bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1122936, 4)\n"
     ]
    }
   ],
   "source": [
    "print((tweets_meta_df.count(), len(tweets_meta_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 11)\n",
      "['userid', 'user_display_name', 'user_screen_name', 'user_reported_location', 'user_profile_description', 'user_profile_url', 'account_creation_date', 'account_language', 'follower_count', 'following_count', 'last_tweet_at']\n"
     ]
    }
   ],
   "source": [
    "print((tweets_user_df.count(), len(tweets_user_df.columns)))\n",
    "print(tweets_user_df.columns)\n",
    "#tweets_user_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the dates and integers present in the dataframe. This also treats the wrong encodings in those columns (such as a language ('en') present in *last_tweet_at*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_user_df = tweets_user_df.withColumn('account_creation_date', to_timestamp(tweets_user_df.account_creation_date))\n",
    "tweets_user_df = tweets_user_df.withColumn('last_tweet_at', to_timestamp(tweets_user_df.last_tweet_at))\n",
    "tweets_user_df = tweets_user_df.withColumn('follower_count', tweets_user_df.follower_count.cast('int'))\n",
    "tweets_user_df = tweets_user_df.withColumn('following_count', tweets_user_df.following_count.cast('int'))\n",
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also appear to be some wrong encodings in *account_language*.\n",
    "## TODO: treat that with the bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660 8\n",
      "+--------------------+--------------------+\n",
      "|              userid|    account_language|\n",
      "+--------------------+--------------------+\n",
      "|4c05300dee83b32e7...|               en-gb|\n",
      "|59889941a89633194...|               en-gb|\n",
      "|76e8b0a247db6b3d2...|               en-gb|\n",
      "|84dd8c4f6d42a7a78...|               en-gb|\n",
      "|943154a86aa64a498...| islami bilgilər ...|\n",
      "|bac526884ab0d54de...|          2017-03-02|\n",
      "|bc1f64b72afcf37d0...|          2017-02-04|\n",
      "|cc524f9726984adac...|               en-gb|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT userid, account_language FROM tweets_user_sql WHERE LENGTH(account_language)>2\")\n",
    "\n",
    "print(tweets_user_df.count(), temp.count())\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split this dataframe into two:\n",
    "* **anonymized_user_df** contains all the users that are anonymized.\n",
    "* **exposed_user_df** contains all the other users.\n",
    "\n",
    "This allows us to drop two columns for the anonymized users, which are a majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660 618 42\n"
     ]
    }
   ],
   "source": [
    "anonymized_user_df = spark.sql(\"SELECT * FROM tweets_user_sql WHERE userid=user_display_name\")\n",
    "exposed_user_df = spark.sql(\"SELECT * FROM tweets_user_sql WHERE NOT userid=user_display_name\")\n",
    "\n",
    "anonymized_user_df = anonymized_user_df.drop('user_display_name', 'user_screen_name')\n",
    "\n",
    "print(tweets_user_df.count(), anonymized_user_df.count(), exposed_user_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
