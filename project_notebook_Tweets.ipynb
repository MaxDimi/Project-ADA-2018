{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project -- Analysing IRA tweets --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import to_timestamp, isnan\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import col, when, length\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # comment if you want to get the warnings from Searborn... ;) \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data directory\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GETTING TO KNOW OUR MAIN DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking IRAN tweets into account or not \n",
    "include_iran = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets thanks to Spark CSV reader \n",
    "\n",
    "if include_iran:\n",
    "    \n",
    "    # combine both RUS and IRAN datasets\n",
    "    include_description = 'RUS & IRAN'\n",
    "    \n",
    "    tweets_text_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_tweet_text.csv')\n",
    "    tweets_stats_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_tweet_stats.csv')\n",
    "    tweets_meta_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_tweet_metadata.csv')\n",
    "    tweets_user_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_user.csv')\n",
    "\n",
    "else:\n",
    "    \n",
    "    # take only RUS datasets\n",
    "    include_description = 'RUS'\n",
    "    \n",
    "    tweets_text_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_tweet_text.csv')\n",
    "    tweets_stats_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_tweet_stats.csv')\n",
    "    tweets_meta_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_tweet_metadata.csv')\n",
    "    tweets_user_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sizes of the datasets : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of troll_tweet_text : (9041308, 3)\n",
      "size of troll_tweet_stats : (9041308, 17)\n",
      "size of troll_tweet_metadata : (9041308, 6)\n",
      "size of troll_user : (3667, 11)\n"
     ]
    }
   ],
   "source": [
    "# look at the raw global datasets' sizes \n",
    "\n",
    "# n : number of observations\n",
    "n_text = tweets_text_df.count()\n",
    "n_stats = tweets_stats_df.count()\n",
    "n_metadata = tweets_meta_df.count()\n",
    "n_users = tweets_user_df.count()\n",
    "\n",
    "# d : dimensionality of the data \n",
    "d_text = len(tweets_text_df.columns)\n",
    "d_stats = len(tweets_stats_df.columns)\n",
    "d_metadata = len(tweets_meta_df.columns)\n",
    "d_users =  len(tweets_user_df.columns)\n",
    "\n",
    "# print the results \n",
    "print(' Sizes of the datasets : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of troll_tweet_text : '+str((n_text,d_text)))\n",
    "print('size of troll_tweet_stats : '+str((n_stats,d_stats)))\n",
    "print('size of troll_tweet_metadata : '+str((n_metadata,d_metadata)))\n",
    "print('size of troll_user : '+str((n_users,d_users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly more than 9M tweets are available from RUS datasets (10M when combining both RUS AND IRAN). If after the cleaning there remains such a large number of data points, it's most likely that our statistical tests will present some significance if there are really underlying correlations, differences ... etc. \n",
    "\n",
    "We can assume, since the number of records for each dataframe is the same and because of the '*a priori*' description of the data, that the rows are ordered in such a way that every tuple of index $i$ in '*troll_tweet_text*' corresponds to the observations at index $i$ for '*troll_tweet_stats*' and '*troll_tweet_metadata*'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041308, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'), ('tweet_language', 'string'), ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_text look like ? size, dtypes\n",
    "print((n_text,d_text))\n",
    "tweets_text_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------+\n",
      "|           tweetid|tweet_language|          tweet_text|\n",
      "+------------------+--------------+--------------------+\n",
      "|877919995476496385|            ru|\"RT @ruopentwit: ...|\n",
      "|492388766930444288|            ru|Серебром отколоко...|\n",
      "|719455077589721089|            bg|@kpru С-300 в Ира...|\n",
      "|536179342423105537|            ru|Предлагаю судить ...|\n",
      "|841410788409630720|            bg|Предостережение а...|\n",
      "|834365760776630272|            ru|Двойная утопия, и...|\n",
      "|577490527299457024|            ru|RT @harkovnews: Н...|\n",
      "|596522755379560448|            ru|RT @NovostiNsk: «...|\n",
      "|567357519547207680|            en|As sun and cloud ...|\n",
      "|665533117369876480|            ru|RT @vesti_news: Ш...|\n",
      "+------------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a quick view\n",
    "tweets_text_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown *tweet_language* can take both the value 'und', or null. We harmonize this column by setting all NaN to 'und'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_df = tweets_text_df.fillna('und',['tweet_language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041308, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'),\n",
       " ('userid', 'string'),\n",
       " ('tweet_time', 'string'),\n",
       " ('in_reply_to_tweetid', 'string'),\n",
       " ('in_reply_to_userid', 'string'),\n",
       " ('quoted_tweet_tweetid', 'string'),\n",
       " ('is_retweet', 'string'),\n",
       " ('retweet_userid', 'string'),\n",
       " ('retweet_tweetid', 'string'),\n",
       " ('quote_count', 'string'),\n",
       " ('reply_count', 'string'),\n",
       " ('like_count', 'string'),\n",
       " ('retweet_count', 'string'),\n",
       " ('hashtags', 'string'),\n",
       " ('urls', 'string'),\n",
       " ('user_mentions', 'string'),\n",
       " ('poll_choices', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_stats look like ? size, dtypes\n",
    "print((n_stats,d_stats))\n",
    "tweets_stats_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a view is not adapted in the current context)\n",
    "# tweets_stats_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the *tweet_time* into Datetime for ease of use, and we cast some columns into integers. We also create a static sql view of the main dataframe on which we can apply our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes transformations : \n",
    "tweets_stats_df = tweets_stats_df.withColumn('tweet_time', to_timestamp(tweets_stats_df.tweet_time))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('quote_count', tweets_stats_df.quote_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('reply_count', tweets_stats_df.reply_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('like_count', tweets_stats_df.like_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('retweet_count', tweets_stats_df.retweet_count.cast('int'))\n",
    "\n",
    "# create a temporary Spark SQL view\n",
    "tweets_stats_df.createOrReplaceTempView(\"tweets_stats_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start splitting the data into smaller dataframes and remove the useless columns for each of those:\n",
    "* **retweets_df** contains all the posts that are retweets.\n",
    "* **replies_df** contains all the posts that are replies to other tweets.\n",
    "* **normal_tweets_df** contains all the other ('normal') posts.\n",
    "\n",
    "**NB:** some tweets have a value for *in_reply_to_userid* while their *in_reply_to_tweetid* is null (however the inverse never happens). Those are either replies to deleted tweets, or mentions of other users that were treated as replies. We decided to consider them as normal tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the sub-dataset : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of retweets : (3333184, 9)\n"
     ]
    }
   ],
   "source": [
    "# RETWEETS\n",
    "retweets_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=True\")\n",
    "\n",
    "# to understand how we selected the columns to remove, uncomment the next two lines\n",
    "# => unique values for the whole column (either null, True , 0 ...)\n",
    "#for col in retweets_df:\n",
    "    #retweets_df.select(col).distinct().show(10)\n",
    "\n",
    "# drop certain features\n",
    "retweets_df = retweets_df.drop('in_reply_to_tweetid', 'in_reply_to_userid', 'is_retweet',\\\n",
    "                               'quote_count', 'reply_count', 'like_count', 'retweet_count',\\\n",
    "                               'poll_choices')\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "n_retweets = retweets_df.count()\n",
    "d_retweets = len(retweets_df.columns)\n",
    "\n",
    "# print the results\n",
    "print(' Size of the sub-dataset : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of retweets : '+str((n_retweets,d_retweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the sub-dataset : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of replies : (266208, 14)\n"
     ]
    }
   ],
   "source": [
    "# REPLIES\n",
    "replies_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=False AND in_reply_to_tweetid IS NOT NULL\")\n",
    "\n",
    "# to understand how we selected the columns to remove, uncomment the next two lines\n",
    "# => unique values for the whole column (either null, True , 0 ...)\n",
    "#for col in replies_df:\n",
    "    #replies_df.select(col).distinct().show(10)\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "replies_df = replies_df.drop('retweet_tweetid', 'retweet_userid', 'is_retweet')\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "n_replies = replies_df.count()\n",
    "d_replies = len(replies_df.columns)\n",
    "\n",
    "# print the results\n",
    "print(' Size of the sub-dataset : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of replies : '+str((n_replies,d_replies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the sub-dataset : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of normal tweets : (5441916, 13)\n"
     ]
    }
   ],
   "source": [
    "# NORMAL\n",
    "normal_tweets_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=False AND in_reply_to_tweetid IS NULL\")\n",
    "\n",
    "# to understand how we selected the columns to remove, uncomment the next two lines\n",
    "# => unique values for the whole column (either null, True , 0 ...)\n",
    "#for col in normal_tweets_df:\n",
    "    #normal_tweets_df.select(col).distinct().show(10)\n",
    "    \n",
    "# record the size of the created sub-dataset\n",
    "normal_tweets_df = normal_tweets_df.drop('in_reply_to_tweetid', 'retweet_tweetid', 'retweet_userid', 'is_retweet')\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "n_normal = normal_tweets_df.count()\n",
    "d_normal = len(normal_tweets_df.columns)\n",
    "\n",
    "# print the results\n",
    "print(' Size of the sub-dataset : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of normal tweets : '+str((n_normal,d_normal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the number of rows correspond and that we did not duplicate or remove any by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9041308 vs. 9041308\n"
     ]
    }
   ],
   "source": [
    "print(str(n_stats)+' vs. '+str(n_retweets+n_normal+n_replies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041308, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'),\n",
       " ('follower_count', 'string'),\n",
       " ('following_count', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('tweet_client_name', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_stats look like ? size, dtypes\n",
    "print((n_metadata,d_metadata))\n",
    "tweets_meta_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "|           tweetid|follower_count|following_count|latitude|longitude| tweet_client_name|\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "|849295393867399169|          4042|           1470|    null|     null|Twitter Web Client|\n",
      "|567280957913587713|           272|            390|    null|     null|          iziaslav|\n",
      "|493095247690612736|            89|            223|    null|     null|          vavilonX|\n",
      "|493892174069903360|            89|            223|    null|     null|          vavilonX|\n",
      "|512503798506721280|            89|            223|    null|     null|          vavilonX|\n",
      "|499624206246871041|            89|            223|    null|     null|          vavilonX|\n",
      "|491828568251707392|            89|            223|    null|     null|          vavilonX|\n",
      "|493768356810731520|            89|            223|    null|     null|          vavilonX|\n",
      "|502221368222814209|            89|            223|    null|     null|          vavilonX|\n",
      "|502380098495213568|            89|            223|    null|     null|          vavilonX|\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a quick view\n",
    "tweets_meta_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous dataset, we cast some columns into integers and we also create a static sql view of the main dataframe on which we can apply our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes transformations : \n",
    "tweets_meta_df = tweets_meta_df.withColumn('follower_count', tweets_meta_df.follower_count.cast('int'))\n",
    "tweets_meta_df = tweets_meta_df.withColumn('following_count', tweets_meta_df.following_count.cast('int'))\n",
    "\n",
    "# NOTE : we do not cast lattitude/longitude columns into integers since we intend to drop both \n",
    "# columns (see below why).\n",
    "\n",
    "# create a temporary Spark SQL view\n",
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the number of rows with a non-null *latitude*/*longitude* combination is very small compared to the size of dataset (less than 0.05%). Furthermore, several of them are repeated. We thus consider it rather useless and prefer dropping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@answer : **REPEATED, is it really BAD ?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of observations for this data set 9041308\n",
      "number of non NULL lattitude records : 4779 and among them 2938 unique coordinates pairs\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT * FROM tweets_meta_sql WHERE latitude IS NOT NULL\")\n",
    "print('total number of observations for this data set '+str(n_metadata))\n",
    "print('number of non NULL lattitude records : '+str(temp.count())+ ' and among them '+str(temp.select('latitude', 'longitude').distinct().count())+' unique coordinates pairs')\n",
    "\n",
    "# drop lattitude and longitude \n",
    "tweets_meta_df = tweets_meta_df.drop('latitude', 'longitude')\n",
    "      \n",
    "# override previous TempView\n",
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main feature that we can use to split the data here is *tweet_client_name*. When we take a closer look to this column, we discover that there are more than 400 values registered. Many of them seem unidentifiable.\n",
    "\n",
    "However, we can see that a good amount of tweets are sent through official Twitter applications:\n",
    "* **Twitter Web Client** accounts for around one third of the tweets in the dataset.\n",
    "* **TweetDeck**, which allows to manage multiple accounts simultaneously, handles around 7% of the tweets.\n",
    "* **Twitter For Android** is also in the top 15 applications used for those tweets.\n",
    "\n",
    "Most of the other tweets are generated through automated social media managers, such as **twitterfeed** (which had been shut down in 2016), **dlvr.it**, or even **IFTTT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of twitter clients : 334\n",
      "+-------------------+-------+\n",
      "|  tweet_client_name|  count|\n",
      "+-------------------+-------+\n",
      "| Twitter Web Client|2576596|\n",
      "|        twitterfeed|1472547|\n",
      "|          TweetDeck| 612024|\n",
      "|      newtwittersky| 393074|\n",
      "|          bronislav| 308516|\n",
      "|           iziaslav| 299963|\n",
      "|              IFTTT| 291269|\n",
      "|          rostislav| 289475|\n",
      "|        generationπ| 285503|\n",
      "|         Twibble.io| 268402|\n",
      "|    Ohwee Messanger| 240051|\n",
      "|NovaPress Publisher| 204583|\n",
      "|Twitter for Android| 163227|\n",
      "|Приложение для тебя| 159588|\n",
      "|           vavilonX| 148744|\n",
      "+-------------------+-------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT tweet_client_name, COUNT(*) AS count FROM tweets_meta_sql GROUP BY tweet_client_name ORDER BY count DESC\")\n",
    "print('number of twitter clients : '+str(temp.count()))\n",
    "temp.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on Twitter clients of the kind : *Twitter ... for ....*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of such typical twitter clients : 18\n",
      "+--------------------+-------+\n",
      "|   tweet_client_name|  count|\n",
      "+--------------------+-------+\n",
      "|  Twitter Web Client|2576596|\n",
      "| Twitter for Android| 163227|\n",
      "|  Twitter for iPhone|  56168|\n",
      "|Twitter for Andro...|  22126|\n",
      "|    Twitter for iPad|   4432|\n",
      "|Twitter for  Android|   3642|\n",
      "|        Twitter Lite|   2891|\n",
      "|      Twitter Nation|    813|\n",
      "|Twitter for Websites|    762|\n",
      "|Twitter for Nokia...|    442|\n",
      "|Twitter for Black...|     92|\n",
      "| Twitter for Windows|     92|\n",
      "|Twitter for Black...|     71|\n",
      "|Twitterrific for iOS|     21|\n",
      "|Twitter for Windo...|      9|\n",
      "|         Twitter Ads|      6|\n",
      "|Twitter Business ...|      2|\n",
      "|Unfollow Tools fo...|      1|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT tweet_client_name, COUNT(*) AS count FROM tweets_meta_sql WHERE tweet_client_name LIKE '%Twitter%' GROUP BY tweet_client_name ORDER BY count DESC\")\n",
    "print('number of such typical twitter clients : '+str(temp.count()))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, seeing how messy this dataset is, and how few columns it has, we decided to not split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of troll_tweet_metadata : (9041308, 4)\n"
     ]
    }
   ],
   "source": [
    "# update the dimensionality of this dataset after the drop of columns\n",
    "d_metadata=  len(tweets_meta_df.columns)\n",
    "print('size of troll_tweet_metadata : '+str((n_metadata,d_metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3667, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('userid', 'string'),\n",
       " ('user_display_name', 'string'),\n",
       " ('user_screen_name', 'string'),\n",
       " ('user_reported_location', 'string'),\n",
       " ('user_profile_description', 'string'),\n",
       " ('user_profile_url', 'string'),\n",
       " ('account_creation_date', 'string'),\n",
       " ('account_language', 'string'),\n",
       " ('follower_count', 'string'),\n",
       " ('following_count', 'string'),\n",
       " ('last_tweet_at', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_stats look like ? size, dtypes\n",
    "print((n_users,d_users))\n",
    "tweets_user_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the dates and integers present in the dataframe. This also treats the wrong encodings in those columns (such as a language ('en') present in *last_tweet_at*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes transformations : \n",
    "tweets_user_df = tweets_user_df.withColumn('account_creation_date', to_timestamp(tweets_user_df.account_creation_date))\n",
    "tweets_user_df = tweets_user_df.withColumn('last_tweet_at', to_timestamp(tweets_user_df.last_tweet_at))\n",
    "tweets_user_df = tweets_user_df.withColumn('follower_count', tweets_user_df.follower_count.cast('int'))\n",
    "tweets_user_df = tweets_user_df.withColumn('following_count', tweets_user_df.following_count.cast('int'))\n",
    "\n",
    "# create a temporary Spark SQL view\n",
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also appear to be some wrong encodings in *account_language*. All languages are represented by a two letters code (except for *en-gb* and *zh-cn*, which correspond respectively to British English and Mainland Chinese). But a very small number of rows contain a date or a text as language.\n",
    "\n",
    "After looking further into that, we discovered that those accounts wrote tweets in many different languages. As it is impossible for us to determine which one is their preferred language, we decided to set those inconsistent values to *'und'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with inconsistent account_language: 3\n",
      "+--------------------+----------------+\n",
      "|              userid|account_language|\n",
      "+--------------------+----------------+\n",
      "|02b81295dbf8951d1...|      2016-01-13|\n",
      "|          1240007161|      2013-03-03|\n",
      "|8e77873eecf19db8d...|      2017-03-21|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT userid, account_language FROM tweets_user_sql WHERE LENGTH(account_language)>5\")\n",
    "print(\"number of rows with inconsistent account_language: \" + str(temp.count()))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the transformations required by the last comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on the account_language feature\n",
    "tweets_user_df = tweets_user_df.withColumn('account_language', when(length(col('account_language'))>5, 'und').otherwise(col('account_language')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary Spark SQL view\n",
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that there are no more 'inconsistent' rows w.r.t our standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with inconsistent account_language: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"number of rows with inconsistent account_language: \" + str(spark.sql(\"SELECT userid, account_language FROM tweets_user_sql WHERE LENGTH(account_language)>5\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split this dataframe into two:\n",
    "* **anonymized_user_df** contains all the users that are anonymized.\n",
    "* **exposed_user_df** contains all the other users.\n",
    "\n",
    "This allows us to drop two columns for the anonymized users (users that have a *userid* that's the same as their *user_display_name* and *user_screen_name* : https://storage.googleapis.com/twitter-election-integrity/hashed/Twitter_Elections_Integrity_Datasets_hashed_README.txt) , which are a majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records from dataset user : 3667\n",
      "wherein there are 3500 anonymized accounts and 167 exposed accounts\n"
     ]
    }
   ],
   "source": [
    "anonymized_user_df = spark.sql(\"SELECT * FROM tweets_user_sql WHERE userid=user_display_name\")\n",
    "exposed_user_df = spark.sql(\"SELECT * FROM tweets_user_sql WHERE NOT userid=user_display_name\")\n",
    "\n",
    "# drop useless columns \n",
    "anonymized_user_df = anonymized_user_df.drop('user_display_name', 'user_screen_name')\n",
    "\n",
    "# print results + check that there are only two outcomes possible for userid = user_display_name\n",
    "print('number of records from dataset user : '+str(n_users))\n",
    "print('wherein there are '+str(anonymized_user_df.count())+' anonymized accounts and '+\\\n",
    "      str(exposed_user_df.count())+' exposed accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have generated 4 main dataframes from the data files: *tweets_text_df*, *tweets_stats_df*, *tweets_meta_df*, and *tweets_user_df*. We then cleaned the inconsistent values and split those dataframes into smaller ones when possible and useful. Our data is now organised as follows:\n",
    "\n",
    "* **tweets_text_df**: all the contents from the tweets, with an indication of their language.\n",
    "* **tweets_stats_df**: \n",
    "    * **retweets_df**: all the information about retweets.\n",
    "    * **replies_df**: all the information about replies.\n",
    "    * **normal_tweets_df**: all the information about the other tweets.\n",
    "* **tweets_meta_df**: all the meta information corresponding to each tweets (minus the latitude/longitude).\n",
    "* **tweets_user_df**:\n",
    "    * **anonymized_user_df**: all the information about anonymized users.\n",
    "    * **exposed_user_df**: all the information about users who are not anonymized.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RETRIEVING DESCRIPTIVE STATISTICS OF OUR DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TACKLING OUR QUESTIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
