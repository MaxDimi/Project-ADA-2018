{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project -- Analysing IRA tweets --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Demo\\Anaconda3\\envs\\ada\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Demo\\Anaconda3\\envs\\ada\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from translate import Translator\n",
    "import spacy\n",
    "import findspark\n",
    "import nltk\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import to_timestamp, isnan\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import col, when, length\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # comment if you want to get the warnings from Searborn... ;) \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data directory\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main parts in this current *Jupyter Notebook* :\n",
    "\n",
    "* 1) **GETTING TO KNOW OUR MAIN DATASETS**. We intend to understand what contain our datasets (features and their dtypes) and to clean them in order to be ready for interpretation. We also may want to split or create new sub-datasets if there are interesting opportunities to do so.\n",
    "\n",
    "\n",
    "* 2) **DESCRIPTIVE STATISTICS AND PROJECT FEASABILITY**. In this part of the notebook we want to give summary statistics about the datasets we will need the most for our project. Furthermore we will try to figure out whether or not our objectives of Milestone 1 are still feasible after the reading of our datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GETTING TO KNOW OUR MAIN DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking IRAN tweets into account or not \n",
    "include_iran = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets thanks to Spark CSV reader \n",
    "\n",
    "if include_iran:\n",
    "    \n",
    "    # combine both RUS and IRAN datasets\n",
    "    include_description = 'RUS & IRAN'\n",
    "    \n",
    "    tweets_text_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_tweet_text.csv')\n",
    "    tweets_stats_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_tweet_stats.csv')\n",
    "    tweets_meta_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_tweet_metadata.csv')\n",
    "    tweets_user_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'*_troll_user.csv')\n",
    "\n",
    "else:\n",
    "    \n",
    "    # take only RUS datasets\n",
    "    include_description = 'RUS'\n",
    "    \n",
    "    tweets_text_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_tweet_text.csv')\n",
    "    tweets_stats_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_tweet_stats.csv')\n",
    "    tweets_meta_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_tweet_metadata.csv')\n",
    "    tweets_user_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_DIR+'rus_troll_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sizes of the datasets : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of troll_tweet_text : (9041308, 3)\n",
      "size of troll_tweet_stats : (9041308, 17)\n",
      "size of troll_tweet_metadata : (9041308, 6)\n",
      "size of troll_user : (3667, 11)\n"
     ]
    }
   ],
   "source": [
    "# look at the raw global datasets' sizes \n",
    "\n",
    "# n : number of observations\n",
    "n_text = tweets_text_df.count()\n",
    "n_stats = tweets_stats_df.count()\n",
    "n_metadata = tweets_meta_df.count()\n",
    "n_users = tweets_user_df.count()\n",
    "\n",
    "# d : dimensionality of the data \n",
    "d_text = len(tweets_text_df.columns)\n",
    "d_stats = len(tweets_stats_df.columns)\n",
    "d_metadata = len(tweets_meta_df.columns)\n",
    "d_users =  len(tweets_user_df.columns)\n",
    "\n",
    "# print the results \n",
    "print(' Sizes of the datasets : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of troll_tweet_text : '+str((n_text,d_text)))\n",
    "print('size of troll_tweet_stats : '+str((n_stats,d_stats)))\n",
    "print('size of troll_tweet_metadata : '+str((n_metadata,d_metadata)))\n",
    "print('size of troll_user : '+str((n_users,d_users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly more than 9M tweets are available from RUS datasets (10M when combining both RUS AND IRAN). If after the cleaning there remains such a large number of data points, it's most likely that our statistical tests will present some significance if there are really underlying correlations, differences ... etc. \n",
    "\n",
    "We can assume, since the number of records for each dataframe is the same and because of the '*a priori*' description of the data, that the rows are ordered in such a way that every tuple of index $i$ in '*troll_tweet_text*' corresponds to the observations at index $i$ for '*troll_tweet_stats*' and '*troll_tweet_metadata*'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041308, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'), ('tweet_language', 'string'), ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_text look like ? size, dtypes\n",
    "print((n_text,d_text))\n",
    "tweets_text_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------+\n",
      "|           tweetid|tweet_language|          tweet_text|\n",
      "+------------------+--------------+--------------------+\n",
      "|877919995476496385|            ru|\"RT @ruopentwit: ...|\n",
      "|492388766930444288|            ru|Серебром отколоко...|\n",
      "|719455077589721089|            bg|@kpru С-300 в Ира...|\n",
      "|536179342423105537|            ru|Предлагаю судить ...|\n",
      "|841410788409630720|            bg|Предостережение а...|\n",
      "|834365760776630272|            ru|Двойная утопия, и...|\n",
      "|577490527299457024|            ru|RT @harkovnews: Н...|\n",
      "|596522755379560448|            ru|RT @NovostiNsk: «...|\n",
      "|567357519547207680|            en|As sun and cloud ...|\n",
      "|665533117369876480|            ru|RT @vesti_news: Ш...|\n",
      "+------------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a quick view\n",
    "tweets_text_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown *tweet_language* can take both the value 'und', or null. We harmonize this column by setting all NaN to 'und'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_df = tweets_text_df.fillna('und',['tweet_language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041308, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'),\n",
       " ('userid', 'string'),\n",
       " ('tweet_time', 'string'),\n",
       " ('in_reply_to_tweetid', 'string'),\n",
       " ('in_reply_to_userid', 'string'),\n",
       " ('quoted_tweet_tweetid', 'string'),\n",
       " ('is_retweet', 'string'),\n",
       " ('retweet_userid', 'string'),\n",
       " ('retweet_tweetid', 'string'),\n",
       " ('quote_count', 'string'),\n",
       " ('reply_count', 'string'),\n",
       " ('like_count', 'string'),\n",
       " ('retweet_count', 'string'),\n",
       " ('hashtags', 'string'),\n",
       " ('urls', 'string'),\n",
       " ('user_mentions', 'string'),\n",
       " ('poll_choices', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_stats look like ? size, dtypes\n",
    "print((n_stats,d_stats))\n",
    "tweets_stats_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a view is not adapted in the current context)\n",
    "# tweets_stats_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the *tweet_time* into Datetime for ease of use, and we cast some columns into integers. We also create a static sql view of the main dataframe on which we can apply our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes transformations : \n",
    "tweets_stats_df = tweets_stats_df.withColumn('tweet_time', to_timestamp(tweets_stats_df.tweet_time))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('quote_count', tweets_stats_df.quote_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('reply_count', tweets_stats_df.reply_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('like_count', tweets_stats_df.like_count.cast('int'))\n",
    "tweets_stats_df = tweets_stats_df.withColumn('retweet_count', tweets_stats_df.retweet_count.cast('int'))\n",
    "\n",
    "# create a temporary Spark SQL view\n",
    "tweets_stats_df.createOrReplaceTempView(\"tweets_stats_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start splitting the data into smaller dataframes and remove the useless columns for each of those:\n",
    "* **retweets_df** contains all the posts that are retweets.\n",
    "* **replies_df** contains all the posts that are replies to other tweets.\n",
    "* **normal_tweets_df** contains all the other ('normal') posts.\n",
    "\n",
    "**NB:** some tweets have a value for *in_reply_to_userid* while their *in_reply_to_tweetid* is null (however the inverse never happens). Those are either replies to deleted tweets, or mentions of other users that were treated as replies. We decided to consider them as normal tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the sub-dataset : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of retweets : (3333184, 9)\n"
     ]
    }
   ],
   "source": [
    "# RETWEETS\n",
    "retweets_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=True\")\n",
    "\n",
    "# to understand how we selected the columns to remove, uncomment the next two lines\n",
    "# => unique values for the whole column (either null, True , 0 ...)\n",
    "#for col in retweets_df:\n",
    "    #retweets_df.select(col).distinct().show(10)\n",
    "\n",
    "# drop certain features\n",
    "retweets_df = retweets_df.drop('in_reply_to_tweetid', 'in_reply_to_userid', 'is_retweet',\\\n",
    "                               'quote_count', 'reply_count', 'like_count', 'retweet_count',\\\n",
    "                               'poll_choices')\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "n_retweets = retweets_df.count()\n",
    "d_retweets = len(retweets_df.columns)\n",
    "\n",
    "# print the results\n",
    "print(' Size of the sub-dataset : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of retweets : '+str((n_retweets,d_retweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the sub-dataset : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of replies : (266208, 14)\n"
     ]
    }
   ],
   "source": [
    "# REPLIES\n",
    "replies_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=False AND in_reply_to_tweetid IS NOT NULL\")\n",
    "\n",
    "# to understand how we selected the columns to remove, uncomment the next two lines\n",
    "# => unique values for the whole column (either null, True , 0 ...)\n",
    "#for col in replies_df:\n",
    "    #replies_df.select(col).distinct().show(10)\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "replies_df = replies_df.drop('retweet_tweetid', 'retweet_userid', 'is_retweet')\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "n_replies = replies_df.count()\n",
    "d_replies = len(replies_df.columns)\n",
    "\n",
    "# print the results\n",
    "print(' Size of the sub-dataset : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of replies : '+str((n_replies,d_replies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the sub-dataset : (RUS)\n",
      " --------------------------------------------------------\n",
      "size of normal tweets : (5441916, 13)\n"
     ]
    }
   ],
   "source": [
    "# NORMAL\n",
    "normal_tweets_df = spark.sql(\"SELECT * FROM tweets_stats_sql WHERE is_retweet=False AND in_reply_to_tweetid IS NULL\")\n",
    "\n",
    "# to understand how we selected the columns to remove, uncomment the next two lines\n",
    "# => unique values for the whole column (either null, True , 0 ...)\n",
    "#for col in normal_tweets_df:\n",
    "    #normal_tweets_df.select(col).distinct().show(10)\n",
    "    \n",
    "# record the size of the created sub-dataset\n",
    "normal_tweets_df = normal_tweets_df.drop('in_reply_to_tweetid', 'retweet_tweetid', 'retweet_userid', 'is_retweet')\n",
    "\n",
    "# record the size of the created sub-dataset\n",
    "n_normal = normal_tweets_df.count()\n",
    "d_normal = len(normal_tweets_df.columns)\n",
    "\n",
    "# print the results\n",
    "print(' Size of the sub-dataset : ('+include_description+')')\n",
    "print(' --------------------------------------------------------')\n",
    "print('size of normal tweets : '+str((n_normal,d_normal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the number of rows correspond and that we did not duplicate or remove any by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9041308 vs. 9041308\n"
     ]
    }
   ],
   "source": [
    "print(str(n_stats)+' vs. '+str(n_retweets+n_normal+n_replies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041308, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'),\n",
       " ('follower_count', 'string'),\n",
       " ('following_count', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('tweet_client_name', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_stats look like ? size, dtypes\n",
    "print((n_metadata,d_metadata))\n",
    "tweets_meta_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "|           tweetid|follower_count|following_count|latitude|longitude| tweet_client_name|\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "|849295393867399169|          4042|           1470|    null|     null|Twitter Web Client|\n",
      "|567280957913587713|           272|            390|    null|     null|          iziaslav|\n",
      "|493095247690612736|            89|            223|    null|     null|          vavilonX|\n",
      "|493892174069903360|            89|            223|    null|     null|          vavilonX|\n",
      "|512503798506721280|            89|            223|    null|     null|          vavilonX|\n",
      "|499624206246871041|            89|            223|    null|     null|          vavilonX|\n",
      "|491828568251707392|            89|            223|    null|     null|          vavilonX|\n",
      "|493768356810731520|            89|            223|    null|     null|          vavilonX|\n",
      "|502221368222814209|            89|            223|    null|     null|          vavilonX|\n",
      "|502380098495213568|            89|            223|    null|     null|          vavilonX|\n",
      "+------------------+--------------+---------------+--------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a quick view\n",
    "tweets_meta_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous dataset, we cast some columns into integers and we also create a static sql view of the main dataframe on which we can apply our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes transformations : \n",
    "tweets_meta_df = tweets_meta_df.withColumn('follower_count', tweets_meta_df.follower_count.cast('int'))\n",
    "tweets_meta_df = tweets_meta_df.withColumn('following_count', tweets_meta_df.following_count.cast('int'))\n",
    "\n",
    "# NOTE : we do not cast lattitude/longitude columns into integers since we intend to drop both \n",
    "# columns (see below why).\n",
    "\n",
    "# create a temporary Spark SQL view\n",
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the number of rows with a non-null *latitude*/*longitude* combination is very small compared to the size of dataset (less than 0.05%). Several of them are repeated. Thus we consider it rather useless and prefer dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of observations for this data set 9041308\n",
      "number of non NULL lattitude records : 4779 and among them 2938 unique coordinates pairs\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT * FROM tweets_meta_sql WHERE latitude IS NOT NULL\")\n",
    "print('total number of observations for this data set '+str(n_metadata))\n",
    "print('number of non NULL lattitude records : '+str(temp.count())+ ' and among them '+str(temp.select('latitude', 'longitude').distinct().count())+' unique coordinates pairs')\n",
    "\n",
    "# drop lattitude and longitude \n",
    "tweets_meta_df = tweets_meta_df.drop('latitude', 'longitude')\n",
    "      \n",
    "# override previous TempView\n",
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main feature that we can use to split the data here is *tweet_client_name*. When we take a closer look to this column, we discover that there are more than 400 values registered. Many of them seem unidentifiable.\n",
    "\n",
    "However, we can see that a good amount of tweets are sent through official Twitter applications:\n",
    "* **Twitter Web Client** accounts for around one third of the tweets in the dataset.\n",
    "* **TweetDeck**, which allows to manage multiple accounts simultaneously, handles around 7% of the tweets.\n",
    "* **Twitter For Android** is also in the top 15 applications used for those tweets.\n",
    "\n",
    "Most of the other tweets are generated through automated social media managers, such as **twitterfeed** (which had been shut down in 2016), **dlvr.it**, or even **IFTTT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of twitter clients : 334\n",
      "+-------------------+-------+\n",
      "|  tweet_client_name|  count|\n",
      "+-------------------+-------+\n",
      "| Twitter Web Client|2576596|\n",
      "|        twitterfeed|1472547|\n",
      "|          TweetDeck| 612024|\n",
      "|      newtwittersky| 393074|\n",
      "|          bronislav| 308516|\n",
      "|           iziaslav| 299963|\n",
      "|              IFTTT| 291269|\n",
      "|          rostislav| 289475|\n",
      "|        generationπ| 285503|\n",
      "|         Twibble.io| 268402|\n",
      "|    Ohwee Messanger| 240051|\n",
      "|NovaPress Publisher| 204583|\n",
      "|Twitter for Android| 163227|\n",
      "|Приложение для тебя| 159588|\n",
      "|           vavilonX| 148744|\n",
      "+-------------------+-------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT tweet_client_name, COUNT(*) AS count FROM tweets_meta_sql GROUP BY tweet_client_name ORDER BY count DESC\")\n",
    "print('number of twitter clients : '+str(temp.count()))\n",
    "temp.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on Twitter clients of the kind : *Twitter ... for ....*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of such typical twitter clients : 18\n",
      "+--------------------+-------+\n",
      "|   tweet_client_name|  count|\n",
      "+--------------------+-------+\n",
      "|  Twitter Web Client|2576596|\n",
      "| Twitter for Android| 163227|\n",
      "|  Twitter for iPhone|  56168|\n",
      "|Twitter for Andro...|  22126|\n",
      "|    Twitter for iPad|   4432|\n",
      "|Twitter for  Android|   3642|\n",
      "|        Twitter Lite|   2891|\n",
      "|      Twitter Nation|    813|\n",
      "|Twitter for Websites|    762|\n",
      "|Twitter for Nokia...|    442|\n",
      "| Twitter for Windows|     92|\n",
      "|Twitter for Black...|     92|\n",
      "|Twitter for Black...|     71|\n",
      "|Twitterrific for iOS|     21|\n",
      "|Twitter for Windo...|      9|\n",
      "|         Twitter Ads|      6|\n",
      "|Twitter Business ...|      2|\n",
      "|Unfollow Tools fo...|      1|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT tweet_client_name, COUNT(*) AS count FROM tweets_meta_sql WHERE tweet_client_name LIKE '%Twitter%' GROUP BY tweet_client_name ORDER BY count DESC\")\n",
    "print('number of such typical twitter clients : '+str(temp.count()))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, seeing how messy this dataset is, and how few columns it has, we decided to not split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of troll_tweet_metadata : (9041308, 4)\n"
     ]
    }
   ],
   "source": [
    "# update the dimensionality of this dataset after the drop of columns\n",
    "d_metadata=  len(tweets_meta_df.columns)\n",
    "print('size of troll_tweet_metadata : '+str((n_metadata,d_metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3667, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('userid', 'string'),\n",
       " ('user_display_name', 'string'),\n",
       " ('user_screen_name', 'string'),\n",
       " ('user_reported_location', 'string'),\n",
       " ('user_profile_description', 'string'),\n",
       " ('user_profile_url', 'string'),\n",
       " ('account_creation_date', 'string'),\n",
       " ('account_language', 'string'),\n",
       " ('follower_count', 'string'),\n",
       " ('following_count', 'string'),\n",
       " ('last_tweet_at', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does tweets_stats look like ? size, dtypes\n",
    "print((n_users,d_users))\n",
    "tweets_user_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there anything to clean at this stage (before playing/dealing with the data) ?*** \n",
    "\n",
    "Below are listed the interventions we needed to perform in order to clean the data :      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the dates and integers present in the dataframe. This also treats the wrong encodings in those columns (such as a language ('en') present in *last_tweet_at*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes transformations : \n",
    "tweets_user_df = tweets_user_df.withColumn('account_creation_date', to_timestamp(tweets_user_df.account_creation_date))\n",
    "tweets_user_df = tweets_user_df.withColumn('last_tweet_at', to_timestamp(tweets_user_df.last_tweet_at))\n",
    "tweets_user_df = tweets_user_df.withColumn('follower_count', tweets_user_df.follower_count.cast('int'))\n",
    "tweets_user_df = tweets_user_df.withColumn('following_count', tweets_user_df.following_count.cast('int'))\n",
    "\n",
    "# create a temporary Spark SQL view\n",
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also appear to be some wrong encodings in *account_language*. All languages are represented by a two letters code (except for *en-gb* and *zh-cn*, which correspond respectively to British English and Mainland Chinese). But a very small number of rows contain a date or a text as language.\n",
    "\n",
    "After looking further into that, we discovered that those accounts wrote tweets in many different languages. As it is impossible for us to determine which one is their preferred language, we decided to set those inconsistent values to *'und'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with inconsistent account_language: 3\n",
      "+--------------------+----------------+\n",
      "|              userid|account_language|\n",
      "+--------------------+----------------+\n",
      "|02b81295dbf8951d1...|      2016-01-13|\n",
      "|          1240007161|      2013-03-03|\n",
      "|8e77873eecf19db8d...|      2017-03-21|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT userid, account_language FROM tweets_user_sql WHERE LENGTH(account_language)>5\")\n",
    "print(\"number of rows with inconsistent account_language: \" + str(temp.count()))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the transformations required by the last comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on the account_language feature\n",
    "tweets_user_df = tweets_user_df.withColumn('account_language', when(length(col('account_language'))>5, 'und').otherwise(col('account_language')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary Spark SQL view\n",
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that there are no more 'inconsistent' rows w.r.t our standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with inconsistent account_language: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"number of rows with inconsistent account_language: \" + str(spark.sql(\"SELECT userid, account_language FROM tweets_user_sql WHERE LENGTH(account_language)>5\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split this dataframe into two:\n",
    "* **anonymized_user_df** contains all the users that are anonymized.\n",
    "* **exposed_user_df** contains all the other users.\n",
    "\n",
    "This allows us to drop two columns for the anonymized users (users that have a *userid* that's the same as their *user_display_name* and *user_screen_name* : [Google APIs : twitter election integrity](https://storage.googleapis.com/twitter-election-integrity/hashed/Twitter_Elections_Integrity_Datasets_hashed_README.txt)) , which are a majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records from dataset user : 3667\n",
      "wherein there are 3500 anonymized accounts and 167 exposed accounts\n"
     ]
    }
   ],
   "source": [
    "anonymized_user_df = spark.sql(\"SELECT * FROM tweets_user_sql WHERE userid=user_display_name\")\n",
    "exposed_user_df = spark.sql(\"SELECT * FROM tweets_user_sql WHERE NOT userid=user_display_name\")\n",
    "\n",
    "# drop useless columns \n",
    "anonymized_user_df = anonymized_user_df.drop('user_display_name', 'user_screen_name')\n",
    "\n",
    "# print results + check that there are only two outcomes possible for userid = user_display_name\n",
    "print('number of records from dataset user : '+str(n_users))\n",
    "print('wherein there are '+str(anonymized_user_df.count())+' anonymized accounts and '+\\\n",
    "      str(exposed_user_df.count())+' exposed accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have generated 4 main dataframes from the data files: *tweets_text_df*, *tweets_stats_df*, *tweets_meta_df*, and *tweets_user_df*. We then cleaned the inconsistent values and split those dataframes into smaller ones when possible and useful. Our data is now organised as follows:\n",
    "\n",
    "* **tweets_text_df**: all the contents from the tweets, with an indication of their language.\n",
    "* **tweets_stats_df**: \n",
    "    * **retweets_df**: all the information about retweets.\n",
    "    * **replies_df**: all the information about replies.\n",
    "    * **normal_tweets_df**: all the information about the other tweets.\n",
    "* **tweets_meta_df**: all the meta information corresponding to each tweets (minus the latitude/longitude).\n",
    "* **tweets_user_df**:\n",
    "    * **anonymized_user_df**: all the information about anonymized users.\n",
    "    * **exposed_user_df**: all the information about users who are not anonymized.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before digging into some statistics we would like to underline the fact we can handle the data in its current size with the use of Spark. Previous requests did not take that much time with Spark SQL and the fact that our dataset is well partionned among several sub-datasets (data chunks) of interest will prevent us to query on too large files ! If we request very intensive computations on the data that embed many and many accesses to these chunks we will either make use of *Parquet* files or we will ***persist()*** Spark dataframes to keep them on top of the memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  DESCRIPTIVE STATISTICS OF OUR DATASETS AND PROJECT FEASABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a short reminder, the questions we would like to address with respect to the milestone 1 were  : \n",
    "* 1) Is there a relation between the candidates popularities and the activities of the trolls?\n",
    "* 2) Did the trolls influence the major events of the campaign? Is it the other way around? \n",
    "* 3) Which subjects are discussed by the trolls, and which semantics do they use?\n",
    "* 4) Which media do they tend to talk about and link in their posts?\n",
    "* 5) Do they tend to show direct support or hatred for specific people?\n",
    "* 6) Did the strategy of the trolls change over time?\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary Spark SQL views of our data chunks :\n",
    "\n",
    "# normal_tweets_df\n",
    "normal_tweets_df.createOrReplaceTempView(\"normal_tweets_sql\")\n",
    "\n",
    "# tweets_text_df\n",
    "tweets_text_df.createOrReplaceTempView(\"tweets_text_sql\")\n",
    "\n",
    "# tweets_meta_df\n",
    "tweets_meta_df.createOrReplaceTempView(\"tweets_meta_sql\")\n",
    "\n",
    "# tweets_user_df\n",
    "tweets_user_df.createOrReplaceTempView(\"tweets_user_sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal and Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first analyze some statistics of *tweets_stats_df* ; that will help us to assess the feasability of the questions 1) and 2) together with extra informations provided by websites (as we mentionned in our first *README* : [cesrusc/election](https://cesrusc.org/election/) , [fivethirtyeight/trump-approval-ratings](uhttps://projects.fivethirtyeight.com/trump-approval-ratings/?ex_cid=rrpromo) , [realclearpolitics/polls](https://www.realclearpolitics.com/epolls/latest_polls/president/#) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5441916, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tweetid', 'string'),\n",
       " ('userid', 'string'),\n",
       " ('tweet_time', 'timestamp'),\n",
       " ('in_reply_to_userid', 'string'),\n",
       " ('quoted_tweet_tweetid', 'string'),\n",
       " ('quote_count', 'int'),\n",
       " ('reply_count', 'int'),\n",
       " ('like_count', 'int'),\n",
       " ('retweet_count', 'int'),\n",
       " ('hashtags', 'string'),\n",
       " ('urls', 'string'),\n",
       " ('user_mentions', 'string'),\n",
       " ('poll_choices', 'string')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display columns and their dtypes of normal tweets \n",
    "print((n_normal,d_normal))\n",
    "normal_tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pre-suppose troll *normal_tweets* that have been collected by  [about_twitter/elections-integrity](https://about.twitter.com/en_us/values/elections-integrity.html#data) are messages advocating, insidiously or not, a preferential political position. At this stage we don't analyze yet the content of these tweets (pure trolls, fake news, researchs ... etc). Note that retweets and replies may have some influence as well on the population opinion. \n",
    "\n",
    "We will investigate this later, let's first show the distribution over time of the number of such *normal_tweets* that may convey the core of the inferences russian trolls wanted to apply on **america's twittosphere**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  year  month\n",
       "0     20  2009      5\n",
       "1      6  2009      6\n",
       "2      8  2009      7\n",
       "3     14  2009      9\n",
       "4      6  2009     10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group and count by month (and year)\n",
    "normal_dates_query = \"\"\"SELECT COUNT(tweetid) AS count, YEAR(tweet_time) AS year, MONTH(tweet_time) AS month\n",
    "                        FROM normal_tweets_sql\n",
    "                        GROUP BY YEAR(tweet_time),MONTH(tweet_time)\n",
    "                        ORDER BY YEAR(tweet_time) ASC, MONTH(tweet_time) ASC\n",
    "                     \"\"\"\n",
    "normal_dates_df = spark.sql(normal_dates_query)\n",
    "\n",
    "# store as a Panda dataframe (very small, absolutely no problem with the memory)\n",
    "normal_dates_pd = normal_dates_df.toPandas()\n",
    "\n",
    "# retrieve (only) the first month and first year\n",
    "date_ref = normal_dates_pd.iloc[0][1:]\n",
    "\n",
    "# print \n",
    "normal_dates_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already observe that for some months the database doesn't contain any '*normal tweet*' ! \n",
    "\n",
    "**NOTE/EDIT** : actually there's only one month missing (August 2009) since there are 109 records and thus 108 records after the inital one whereas there would have been 109 records after May 2009 if every month is mentionned in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of months that separate two dates (YEAR,MONTH)\n",
    "def months_space(tuple_now,tuple_start):\n",
    "    '''\n",
    "    INPUT\n",
    "    -----\n",
    "    tuple_now : columns with the date (year,month) (int,int) to be tested\n",
    "    tuple_start : tuple with the reference date (year,month) (int,int) \n",
    "    \n",
    "    OUTPUT\n",
    "    ------\n",
    "    number of months 'elapsed' for instance, months_space((2013,1),(2012,3)) = 10 \n",
    "                                             months_space((2015,4),(2015,3)) = 1\n",
    "    '''\n",
    "    # retrieve years and months\n",
    "    year_now = tuple_now[:,0]\n",
    "    month_now = tuple_now[:,1]\n",
    "    year_start = tuple_start[0]\n",
    "    month_start = tuple_start[1]\n",
    "    \n",
    "    # return the number of months separating the two dates\n",
    "    return 12*(year_now-year_start) + (month_now-month_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting purposes, let's add another column to the Pandas dataframe which gives the spacing (in terms of number of months) between the months where troll tweets were collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the new function months_space\n",
    "normal_dates_pd['months spacing'] = months_space(normal_dates_pd[['year','month']].values,date_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary Spark SQL view in order to select only the normal tweets\n",
    "# whose dates correspond the US presidential campaign\n",
    "normal_dates_df.createOrReplaceTempView(\"normal_dates_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first normal tweet date (unix_time stamp) : 1241877540\n",
      "first normal tweet date (human readable) : 2009-05-09 13:59:00\n"
     ]
    }
   ],
   "source": [
    "# recover the date of the first troll normal_tweet identified\n",
    "start_normal_date_query = \"\"\"SELECT MIN(UNIX_TIMESTAMP(tweet_time)) FROM normal_tweets_sql\"\"\"\n",
    "\n",
    "start_normal_date = spark.sql(start_normal_date_query)\n",
    "start_normal_date = start_normal_date.head()[0] # access the value of the query \n",
    "\n",
    "print('first normal tweet date (unix_time stamp) : ' + str(start_normal_date))\n",
    "print('first normal tweet date (human readable) : '+str(pd.to_datetime(start_normal_date,unit='s')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That corresponds to a tweet back in 2009 (9th of may). As the presidential campaign didn't really begin so early we will certainly introduce a threshold of validity based on the date of apparition for date-based studies/researchs. \n",
    "\n",
    "The reason why we observe such timestamps could be :\n",
    "\n",
    "* weakness of the filter for troll data collection (false positive troll tweets)\n",
    "* tweak (did hackers manage to mislead data collectors by faking the date of their tweets?)\n",
    "* just another bug while recording the informations about the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot purposes (whole data without truncating and\n",
    "# without keeping only observations during the presidential campaign)\n",
    "plt.figure(1)\n",
    "sns.scatterplot(x = \"months spacing\", y=\"count\", data=normal_dates_pd)\n",
    "plt.title('Evolution with the time of the number of detected troll normal tweets')\n",
    "plt.grid()\n",
    "plt.xlabel('months spacing')\n",
    "plt.ylabel('count')\n",
    "plt.legend(['bar plot']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a peak starting after 61 months from May 2009 , thus in June 2014. This corresponds exactly to one year prior the moment when Donald Trump announced his candidacy for the primaries (republican) [wikipedia/donald_trump](https://en.wikipedia.org/wiki/Donald_Trump#Political_activities_up_to_2015). \n",
    "\n",
    "We'll have to determine whether the ***effective*** troll campaign did really (if it existed) start that month ; maybe to trigger in advance the aimed public.\n",
    "\n",
    "After the peak the *troll* fever seems to drop monotically until 85 months after the very first tweets : it corresponds to June 2016 before growing up again **right before the final US elections** of november 2016). \n",
    "\n",
    "Trolls end after 100 months, at the beginning of the academic year 2017-2018 (Sep. 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => time window :  2014 -  2016\n",
    "\n",
    "For the current Milestone, let's focus on that period of time since it corresponds to the whole presidential campaign (+ some time before as a flourishing speculative period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some basic statistics about the feature *retweet_count* of that same *normal_tweets* dataframe. It could help us for our future analysis to evaluate/assess the importance of troll tweets. We do not dispose of view counts (at least for now) for such tweets but what can be sure is that there exists a correlation between the number of views and the number of retweets (the more a post is retweeted, the more likely that it reaches more and more people across the internet).\n",
    "\n",
    "We can compare the summary statistics of *retweet_count* of normal tweets with statistics of typical 'popular' tweets on Twitter to get a kind of 'popularity scale'. Of course we have to be careful with the data we're dealing with : there might be some automatic retweet nested chains that would bias those results. We'll have to inform ourselves slightly more about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for RT counts for normal_tweets : \n",
      "----------------------------------------------------\n",
      "+-------+-----------------+\n",
      "|summary|         retweets|\n",
      "+-------+-----------------+\n",
      "|  count|          4034671|\n",
      "|   mean|3.648541603516123|\n",
      "| stddev|84.69370646803212|\n",
      "|    min|                0|\n",
      "|    max|            97498|\n",
      "+-------+-----------------+\n",
      "\n",
      "approx. 1st quarter quantile : 0.0 for relative error of 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-572c9cdadbb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mnormal_retweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'approx. 1st quarter quantile : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_retweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweets'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m      \u001b[1;34m' for relative error of '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'approx. median : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_retweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweets'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m      \u001b[1;34m' for relative error of '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'approx. 3rd quarter quantile : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_retweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweets'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m      \u001b[1;34m' for relative error of '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[1;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[0;32m   1742\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1744\u001b[1;33m         \u001b[0mjaq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1745\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m~\\Documents\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get only the retweets counts (for years of interest, @tune)\n",
    "normal_retweets_query = \"\"\"SELECT retweet_count AS retweets\n",
    "                        FROM normal_tweets_sql\n",
    "                        WHERE YEAR(tweet_time)<2017 AND YEAR(tweet_time)>2013\n",
    "                     \"\"\"\n",
    "normal_retweets_df = spark.sql(normal_retweets_query)\n",
    "\n",
    "# relative error tolerance for median computation \n",
    "tol = 0.001\n",
    "\n",
    "print('Summary statistics for RT counts for normal_tweets : ')\n",
    "print('----------------------------------------------------')\n",
    "normal_retweets_df.select('retweets').describe().show()\n",
    "print('approx. 1st quarter quantile : '+str(normal_retweets_df.approxQuantile('retweets',[0.25],tol)[0])+\\\n",
    "      ' for relative error of '+str(tol))\n",
    "print('approx. median : '+str(normal_retweets_df.approxQuantile('retweets',[0.5],tol)[0])+\\\n",
    "      ' for relative error of '+str(tol))\n",
    "print('approx. 3rd quarter quantile : '+str(normal_retweets_df.approxQuantile('retweets',[0.75],tol)[0])+\\\n",
    "      ' for relative error of '+str(tol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We presume that the vast majority of the normal tweets aren't given any RT!**\n",
    "\n",
    "The *stddev* is huge, those statistics seem to indicate a ***power law*** distribution. \n",
    "\n",
    "Let's get the count distribution of the number of RT's. It fits into the memory since we only consider one column of $\\simeq$ 5M observations. Nevertheless we won't store the Pandas for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's discover the number of normal tweets that were RT more than 10 times\n",
    "normal_retweets_sup1_query = \"\"\"SELECT retweet_count AS retweets\n",
    "                        FROM normal_tweets_sql\n",
    "                        WHERE retweet_count>10 AND YEAR(tweet_time)<2017 AND YEAR(tweet_time)>2013\n",
    "                     \"\"\"\n",
    "normal_retweets_sup1_df = spark.sql(normal_retweets_sup1_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => RT count distribution\n",
    "\n",
    "## log scale - truncated\n",
    "\n",
    "# plot purpose  \n",
    "plt.figure(1)\n",
    "nbins_global = 100\n",
    "sns.distplot(normal_retweets_sup1_df.select('retweets').toPandas().apply(lambda x: np.log10(x)),\\\n",
    "             bins = nbins_global, norm_hist = False, kde = False)\n",
    "plt.title('Distribution of the frequency of the number of retweets for normal tweets : ' +\\\n",
    "          str(nbins_global)+' bins')\n",
    "plt.xlabel('# of retweets (log scale)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.legend(['histogram']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of normal tweets that have been only RT less than 10 times is gigantic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the required proportion\n",
    "n_normal_RT_less10 = normal_retweets_df.count() - normal_retweets_sup1_df.count()\n",
    "\n",
    "# print the results\n",
    "print('proportion of normal tweets RT less than 10 times : '+str(n_normal_RT_less10/n_normal * 100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to worry already : this is not sufficient evidence that there were no political inference through troll tweets during the US campaign of 2016. \n",
    "\n",
    "***But it actually may sustain the hypothesis of the existence of clever RT chains that relay the troll across the web at 'low cost' in terms of RTs.***\n",
    "\n",
    "We will investigate why there are for the vast majority of the normal tweets, at first sight, so few RT's and if this phenomenon also arises on the reduced time windows mentionned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @continue.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still it can be interesting to extract the most RT normal tweets and have a look at their content (we keep the *tweetid*'s). \n",
    "\n",
    "We may also want to discover what is the langage used and to which accounts are linked these tweets (we also keep *userid*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1000 (it could be more or less) most RT tweets\n",
    "\n",
    "normal_retweets_top_query = \"\"\"SELECT tweetid as id, userid AS user,retweet_count AS retweets,tweet_time\n",
    "                        FROM normal_tweets_sql\n",
    "                        WHERE YEAR(tweet_time)<2017 AND YEAR(tweet_time)>2013 \n",
    "                        ORDER BY retweet_count DESC\n",
    "                        LIMIT 1000\n",
    "                     \"\"\"\n",
    "normal_retweets_top_df = spark.sql(normal_retweets_top_query)\n",
    "\n",
    "# in order to have direct access on the computed top RT tweets (COMMENT IF LARGE TOP)\n",
    "normal_retweets_top_df.persist()\n",
    "\n",
    "# create temporary Spark SQL view \n",
    "normal_retweets_top_df.createOrReplaceTempView(\"normal_retweets_top_sql\")\n",
    "\n",
    "normal_retweets_top_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join to this table some features of interest as the language of the tweet, the self-reported location of the author (***maybe highly corrupted***), whether or not the tweet did embed a poll plug-in (this feature is maybe more important than *a priori* believed : answering a poll insidiously bias people) , the content of the tweet and finally author's number of followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple JOIN instruction\n",
    "informative_table_top_query = \"\"\"\n",
    "WITH tweets_stats_content AS (SELECT id, tweet_language, user, retweets ,tweet_text,tweet_time FROM normal_retweets_top_sql JOIN tweets_text_sql ON normal_retweets_top_sql.id = tweets_text_sql.tweetid),\n",
    "all_but_localisation AS (SELECT tweetid, tweet_language, user, retweets ,tweet_text, follower_count as follow_numb, following_count as follow_cnt, tweet_client_name,tweet_time FROM tweets_stats_content JOIN tweets_meta_sql ON tweets_stats_content.id = tweets_meta_sql.tweetid),\n",
    "all AS (SELECT user_reported_location, tweetid, tweet_language, user, retweets ,tweet_text, follower_count, following_count, tweet_client_name , tweet_time FROM all_but_localisation JOIN tweets_user_sql ON all_but_localisation.user = tweets_user_sql.userid)\n",
    "SELECT user_reported_location, tweetid, tweet_language, user, retweets ,tweet_text, follower_count, following_count, tweet_client_name , YEAR(tweet_time) AS year, MONTH(tweet_time) AS MONTH FROM all\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the last result into a Parquet file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment if already computed \n",
    "spark.sql(informative_table_top_query).write.mode('overwrite').parquet(\"informative_table_top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the previously built Spark data frame in Parquet's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read Parquet file\n",
    "informative_table_top_df = spark.read.parquet(\"informative_table_top\")\n",
    "\n",
    "# load into Pandas (comment if large top)\n",
    "informative_table_top_pd = informative_table_top_df.toPandas()\n",
    "\n",
    "# quick review\n",
    "informative_table_top_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the last table there seem to be really intricating tweets ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user spotted as being a troll but doesn't have necessarily any link with russia\n",
    "troll_example_df = informative_table_top_pd[informative_table_top_pd.user=='4224729994']\n",
    "troll_example_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of two troll tweets that may have some psychological impact on readers\n",
    "troll_example = troll_example_df.loc[38]\n",
    "troll_example2 = troll_example_df.loc[4]\n",
    "\n",
    "# their content speaks for itself ...\n",
    "troll_example['tweet_text']\n",
    "troll_example2['tweet_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets were produced during the presidential campaign (September 2016).\n",
    "___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at summary statistics about features of our dataframe (fields of interest of the most RT tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics of numerical fields\n",
    "# (other than year or months that can ben considered as categorical in some sense)\n",
    "informative_table_top_pd[['retweets','follower_count','following_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that the most RT tweets among our databases of troll tweets during the US presidential are produced by quite influent Twitter Accounts (median $\\simeq 50$k). \n",
    "\n",
    "We've spent more time on exploring the questions 3) and 4) but we will further investigate this dataframe because it contains precious informations. Of course, it's likely that we're gonna choose not only the 1000 top RT normal tweets but maybe far more. \n",
    "\n",
    "In that latter case some of the previous computations may require Ada's cluster, without being certain for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will investigate possible correlations betwen the number of normal tweets (of a certain kind; a filter has still to be constructed to assess the relevancy of those tweets) with Trump's popularity during the time window June 2014 - September 2017.** Therefore we first have to enrich our datasets with the popularity scores of Trump (and actually other opponents as there might be troll tweets aimed to discredit other candidates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate correlation between number of normal tweets with Trump's\n",
    "# popularity during that time window (June 2014-September 2017)\n",
    "\n",
    "# @ complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will try to catch where do the troll tweets come from and where do they spead.** To this purpose we also have to enrich our dataset with some studies. But unlike the previous point, we are not sure about the existence of such piece of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dig further into the 'importance' assessment of our troll tweets / try to figure out which \n",
    "# population had been 'touched' by the trolls \n",
    "# => we will try to infere this from the reported locations but some extra readings/thoughts are\n",
    "# needed to ensure the validity of this process.\n",
    "\n",
    "# @complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have all the tools to answer to questions 1) and 2) properly thanks to the web-pages informed above. \n",
    "\n",
    "Only the last sub-question (actually not part of 1) nor 2), it's an extra) about the origin of troll tweets will be a bottleneck to overcome looking at more ressources on the internet ; if no satisfying ressource(s) is/are found, we will probably not answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual analysis\n",
    "In order to answer to the questions 3) and 4) , or at least, to have a better insight on what we can do in the future, we must interest ourselves in the text of the tweets itself.\n",
    "\n",
    "**Languages**\n",
    "\n",
    "The first natural step is to identify the different languages present in our database. Let's group the tweets by languages and see which languages we will have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_df.createOrReplaceTempView(\"tweets_text_sql\") # Create temporary view to perform SQL operations\n",
    "\n",
    "query_language = \"\"\"WITH lang_counts AS (SELECT tweet_language, COUNT(tweet_language) AS language_count FROM tweets_text_sql GROUP BY tweet_language)\n",
    "SELECT * FROM lang_counts ORDER BY language_count DESC LIMIT 20\"\"\"\n",
    "\n",
    "language_count_df = spark.sql(query_language)\n",
    "\n",
    "language_count_df.show()\n",
    "\n",
    "# We can then draw a bar plot\n",
    "language_count_df.toPandas().plot.bar(x = \"tweet_language\", y = \"language_count\",legend=False, \n",
    "                                      title=\"The 20 most frequent languages in tweets\",grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a big variety of languages but the two big winners are the 'english' and the 'russian'. We will focus on those two for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to identify the main subjects discussed in each tweet. Unfortunately none of us can speak russian, we nevetherless can translate these tweets with py-translate (Note that the translation is far from being perfect but at least we will have a global idea of what the tweets talk about)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us restrict ourselves to the english and russian tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ru = spark.sql(\"\"\"SELECT * FROM tweets_text_sql WHERE tweet_language = 'ru'\"\"\")\n",
    "df_en = spark.sql(\"\"\"SELECT * FROM tweets_text_sql WHERE tweet_language = 'en'\"\"\")\n",
    "df_ru.show()\n",
    "df_en.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us translate the russian tweets into english !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator= Translator(from_lang=\"russian\",to_lang=\"english\") # Initialise the russian translator\n",
    "df_ru_translated = df_ru.rdd.map(lambda r: (r[0],r[1],translator.translate(r[2]))) # Translate the tweets\n",
    "\n",
    "df_ru_translated = spark.createDataFrame(df_ru_translated) # New dataframe\n",
    "df_ru_translated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translators in python are all querying translation websites. They don't like us to ask them to translate 5 000 000 tweets. This is why they ban our ID after too many translations. For Milestone 3, we should use the free trial google translation API to translate our tweets, save them into a parquet and cancel our free trial offer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this milestone we will thus just focus on the english tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashtags**\n",
    "\n",
    "Another important aspect of the tweets are the hashtags. They represent a lot of information as they can tell about the subject of the tweet, the opinion of the tweeter, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce a pattern to match into the form of a regexp\n",
    "regexp = re.compile(\"\\B#\\w\\w+\",re.IGNORECASE)\n",
    "\n",
    "# @goal : return all the matching expressions : i.e. '#word will be transformed to ['word']\n",
    "def tag_split(text):\n",
    "    return regexp.findall(text)\n",
    "\n",
    "# Apply this first map in order to flatten expressions of the type #word1#word2 ... \n",
    "flattered_tweets = df_en.rdd.map(lambda r: (r[0],r[1],tag_split(r[2])))\n",
    "\n",
    "# Going back to dataframes to enjoy the \"explode\" methods that does exactly what we need\n",
    "tweets_new_df = spark.createDataFrame(flattered_tweets, ['tweetid','tweet_language','tweet_text'])\n",
    "\n",
    "# Create another SQL view for questions with this new representation\n",
    "tweets_new_df.withColumn('hashtag', explode(tweets_new_df.tweet_text)).createOrReplaceTempView(\"tweets_flat_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 20 most frequent hashtags\n",
    "pre_query = \"\"\"WITH hashtag_freq AS(SELECT hashtag, COUNT(tweetid) AS frequency FROM tweets_flat_sql GROUP BY hashtag)\n",
    "SELECT * FROM hashtag_freq ORDER BY frequency DESC LIMIT(20)\"\"\"\n",
    "\n",
    "hashtags_count_df = spark.sql(pre_query)\n",
    "hashtags_count_df.show()\n",
    "\n",
    "# Plot purposes\n",
    "hashtags_count_df.toPandas().plot.bar(x = \"hashtag\", y = \"frequency\",legend=False, title=\"The 20 most dominant hashtags\",grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the most used hashtags are those about news, sports and politics. This makes sense as these 3 topics are actively debated by all tweeter's users, trolls included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words**\n",
    "\n",
    "Now that we have analysed the hashtags, let us focus on the words themselves. This allows to see what are the typical words used by trolls. A comparison between most frequent words between troll and normal tweets is one of the nice plus that we could do for Milestone 3.\n",
    "\n",
    "Note : we will remove the hastags words for this analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first clean the tweets. Indeed a tweet contains a lot of noisy information i.e. links, RT, @somebody, #word, ...\n",
    "\n",
    "Furthermore, a lot of variants of the same words exists. This is why we can use stemming to bring every word into its most reduced form.\n",
    "\n",
    "Finally, every tweet contains stopwords i.e. words that we must use to build sentence but don't bring any particuliar information on the tweet itself (e.g. 'it', 'me', 'then', ...). These words can be removed to do some particuliar analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : *'Hardcoding stopwords may seem to be bad practice but actually these are really problem specific and stackoverflow actually advised us to do it'*\n",
    "\n",
    "[StackOverflow](https://stackoverflow.com/questions/19130512/stopword-removal-with-nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.SnowballStemmer(\"english\")  # Initialize stemmer that 'kills' also stopwords\n",
    "# custom list of stopwords\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
    "             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "             'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "             'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "             'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "             'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn',\n",
    "             'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'make', 'go', 'get', \"it's\", 'us', '&amp;',\n",
    "            'want', 'get', 'new', \"don't\", \"i'm\", 'one', 'say']\n",
    "\n",
    "def tweet_cleaner(text, stem_option = True):\n",
    "    \"\"\"This function takes a String as input and returns a 'cleaned' version of it.\n",
    "    -> No punctuation\n",
    "    -> No links, hashtags, ...\n",
    "    -> Stemming if stem_option = True\"\"\"\n",
    "    text = re.sub(r'[.,\"!,-]+', '', text, flags=re.MULTILINE)  # removes the characters specified\n",
    "    text = re.sub(r'^RT[\\s]+', '', text, flags=re.MULTILINE)  # removes RT\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)  # remove link\n",
    "    text = re.sub(r'[:]+', '', text, flags=re.MULTILINE)\n",
    "    new_line = ''\n",
    "    for i in text.split():  # remove @ and # words\n",
    "        stemmed_word = stemmer.stem(i)\n",
    "        if not i.startswith('@') and not i.startswith('#') and stemmed_word not in stopwords:\n",
    "            new_line += stemmed_word + ' '\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattered_tweets = df_en.rdd.map(lambda r: (r[0],r[1],tweet_cleaner(r[2]).split()))\n",
    "\n",
    "# Going back to dataframes to enjoy the \"explode\" methods that does exactly what we need\n",
    "tweets_new_df = spark.createDataFrame(flattered_tweets, ['tweetid','tweet_language','tweet_text'])\n",
    "\n",
    "# Create another SQL view for questions with this new representation\n",
    "tweets_new_df.withColumn('words', explode(tweets_new_df.tweet_text)).createOrReplaceTempView(\"tweets_flat_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 20 most frequent words\n",
    "pre_query = \"\"\"WITH word_freq AS(SELECT words, COUNT(tweetid) AS frequency FROM tweets_flat_sql GROUP BY words)\n",
    "SELECT * FROM word_freq ORDER BY frequency DESC LIMIT(20)\"\"\"\n",
    "\n",
    "words_count_df = spark.sql(pre_query)\n",
    "words_count_df.show()\n",
    "\n",
    "# Plot purposes\n",
    "words_count_df.toPandas().plot.bar(x = \"words\", y = \"frequency\",legend=False, title=\"The 20 most dominant words\",grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any surprise the most frequent word is trump. Some other interesting words also appear such as \n",
    "* obama : Obama was a great supporter of the main opponent of Trump that is Hillary Clinton. A theory would be that the trolls tried to discredit Obama's opinion\n",
    "* black : A word with a negative connotation. Trump supporters are considered by many to be racist.\n",
    "* kill : Again another word with negative connotation. A lot of interpretations are possible here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic analysis**\n",
    "\n",
    "We can now finally try to analyse the construction of each tweet, what it talks about, the subjects of each tweet.\n",
    "\n",
    "Let us focus here on a toy example of 1000 tweets and clean those tweets (conserving the stopwords and not stemming !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.createOrReplaceTempView(\"tweets_text_en_sql\") # Create temporary view to perform SQL operations\n",
    "toy_example = spark.sql(\"\"\"SELECT tweet_text FROM tweets_text_en_sql LIMIT 1000\"\"\")\n",
    "\n",
    "toy_example_pd = toy_example.toPandas()\n",
    "print('Tweets before cleaning : ' + str(toy_example_pd) + '\\n')\n",
    "toy_example_cleaned = toy_example_pd.apply(lambda x : tweet_cleaner(x, stem_option = False))\n",
    "print('Tweets after cleaning : ' + str(toy_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the data is now much more suitable for performing text processing operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library we will use is called spacy. It is an open-source library that provides a bunch of text-processing functions (these operations are performed using neural networks).\n",
    "\n",
    "The main operation that the library provides is the **nlp function** which allows us to derive what the tweet is talking about by identifying identities such as PERSON, ORG, DATE, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en') # 'en' for english analysis\n",
    "\n",
    "# Loop through every tweet and perfrom text analysis\n",
    "for i, tweet in enumerate(toy_cleaned):\n",
    "    print(tweet)\n",
    "    print('Analysis of tweet ' + str(i) + '\\n')\n",
    "    analysis = nlp(tweet)\n",
    "    # Find named entities, phrases and concepts\n",
    "    for entity in analysis.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the semantic analysis is (barely) not working ! The tweets are just too short sentences and/or are using too many links/hashtags, getting in the way of the global understanding of the tweet.\n",
    "\n",
    "Studying the semantic of the tweets is therefore a (too) complicated operation, which may be dropped for next milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support or hate ?\n",
    "\n",
    "This question is quite difficult to answer such prematurely. However, what we can do for now is what is called a ***sentiment analysis***. This problem is a machine learning classification problem that tries to classifiy a tweet as 'positive' or 'negative'. While it doesn't totally answer to the question 5), it is still a good start that could be reused afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of a sentiment analysis is to define words that we consider to be respectively positive, negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "    words = []\n",
    "    for word in f:\n",
    "        words += [word[:len(word)-2]]\n",
    "    f.close()\n",
    "    return words\n",
    "\n",
    "\n",
    "neg_words = read_file('negative.txt')\n",
    "print('Negative words : ' + str(neg_words))\n",
    "pos_words = read_file('positive.txt')\n",
    "print('Positive words : ' + str(pos_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
